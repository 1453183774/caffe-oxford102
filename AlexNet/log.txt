libdc1394 error: Failed to initialize libdc1394
I0917 22:16:39.597970 67440 caffe.cpp:113] Use GPU with device ID 0
I0917 22:16:39.876261 67440 caffe.cpp:121] Starting Optimization
I0917 22:16:39.876379 67440 solver.cpp:32] Initializing solver from parameters: 
test_iter: 124
test_interval: 500
base_lr: 0.001
display: 50
max_iter: 50000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 20000
snapshot: 10000
snapshot_prefix: "snapshot"
solver_mode: GPU
net: "train_val.prototxt"
I0917 22:16:39.876412 67440 solver.cpp:70] Creating training net from net file: train_val.prototxt
E0917 22:16:39.877084 67440 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: train_val.prototxt
I0917 22:16:39.877429 67440 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
I0917 22:16:39.877526 67440 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0917 22:16:39.877559 67440 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0917 22:16:39.877769 67440 net.cpp:42] Initializing net from parameters: 
name: "Oxford102_CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "/home/ubuntu/caffe/data/ilsvrc12/imagenet_mean.binaryproto"
  }
  image_data_param {
    source: "/home/ubuntu/git/caffe-oxford102/train.txt"
    batch_size: 50
    new_height: 256
    new_width: 256
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_oxford_102"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8_oxford_102"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 102
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_oxford_102"
  bottom: "label"
}
I0917 22:16:39.877908 67440 layer_factory.hpp:74] Creating layer data
I0917 22:16:39.877935 67440 net.cpp:84] Creating Layer data
I0917 22:16:39.877948 67440 net.cpp:338] data -> data
I0917 22:16:39.877972 67440 net.cpp:338] data -> label
I0917 22:16:39.877989 67440 net.cpp:113] Setting up data
I0917 22:16:39.878000 67440 image_data_layer.cpp:36] Opening file /home/ubuntu/git/caffe-oxford102/train.txt
I0917 22:16:39.878551 67440 image_data_layer.cpp:51] A total of 1020 images.
I0917 22:16:39.886176 67440 image_data_layer.cpp:80] output data size: 50,3,227,227
I0917 22:16:39.886207 67440 data_transformer.cpp:22] Loading mean file from: /home/ubuntu/caffe/data/ilsvrc12/imagenet_mean.binaryproto
I0917 22:16:39.895825 67440 net.cpp:120] Top shape: 50 3 227 227 (7729350)
I0917 22:16:39.895866 67440 net.cpp:120] Top shape: 50 (50)
I0917 22:16:39.895879 67440 layer_factory.hpp:74] Creating layer conv1
I0917 22:16:39.895905 67440 net.cpp:84] Creating Layer conv1
I0917 22:16:39.895915 67440 net.cpp:380] conv1 <- data
I0917 22:16:39.895931 67440 net.cpp:338] conv1 -> conv1
I0917 22:16:39.895948 67440 net.cpp:113] Setting up conv1
I0917 22:16:39.950480 67440 net.cpp:120] Top shape: 50 96 55 55 (14520000)
I0917 22:16:39.950531 67440 layer_factory.hpp:74] Creating layer relu1
I0917 22:16:39.950549 67440 net.cpp:84] Creating Layer relu1
I0917 22:16:39.950557 67440 net.cpp:380] relu1 <- conv1
I0917 22:16:39.950568 67440 net.cpp:327] relu1 -> conv1 (in-place)
I0917 22:16:39.950579 67440 net.cpp:113] Setting up relu1
I0917 22:16:39.950728 67440 net.cpp:120] Top shape: 50 96 55 55 (14520000)
I0917 22:16:39.950742 67440 layer_factory.hpp:74] Creating layer pool1
I0917 22:16:39.950755 67440 net.cpp:84] Creating Layer pool1
I0917 22:16:39.950762 67440 net.cpp:380] pool1 <- conv1
I0917 22:16:39.950773 67440 net.cpp:338] pool1 -> pool1
I0917 22:16:39.950783 67440 net.cpp:113] Setting up pool1
I0917 22:16:39.950850 67440 net.cpp:120] Top shape: 50 96 27 27 (3499200)
I0917 22:16:39.950858 67440 layer_factory.hpp:74] Creating layer norm1
I0917 22:16:39.950887 67440 net.cpp:84] Creating Layer norm1
I0917 22:16:39.950904 67440 net.cpp:380] norm1 <- pool1
I0917 22:16:39.950913 67440 net.cpp:338] norm1 -> norm1
I0917 22:16:39.950925 67440 net.cpp:113] Setting up norm1
I0917 22:16:39.950938 67440 net.cpp:120] Top shape: 50 96 27 27 (3499200)
I0917 22:16:39.950945 67440 layer_factory.hpp:74] Creating layer conv2
I0917 22:16:39.950958 67440 net.cpp:84] Creating Layer conv2
I0917 22:16:39.950964 67440 net.cpp:380] conv2 <- norm1
I0917 22:16:39.950973 67440 net.cpp:338] conv2 -> conv2
I0917 22:16:39.950984 67440 net.cpp:113] Setting up conv2
I0917 22:16:39.961284 67440 net.cpp:120] Top shape: 50 256 27 27 (9331200)
I0917 22:16:39.961305 67440 layer_factory.hpp:74] Creating layer relu2
I0917 22:16:39.961318 67440 net.cpp:84] Creating Layer relu2
I0917 22:16:39.961324 67440 net.cpp:380] relu2 <- conv2
I0917 22:16:39.961333 67440 net.cpp:327] relu2 -> conv2 (in-place)
I0917 22:16:39.961343 67440 net.cpp:113] Setting up relu2
I0917 22:16:39.961401 67440 net.cpp:120] Top shape: 50 256 27 27 (9331200)
I0917 22:16:39.961411 67440 layer_factory.hpp:74] Creating layer pool2
I0917 22:16:39.961421 67440 net.cpp:84] Creating Layer pool2
I0917 22:16:39.961427 67440 net.cpp:380] pool2 <- conv2
I0917 22:16:39.961436 67440 net.cpp:338] pool2 -> pool2
I0917 22:16:39.961444 67440 net.cpp:113] Setting up pool2
I0917 22:16:39.961587 67440 net.cpp:120] Top shape: 50 256 13 13 (2163200)
I0917 22:16:39.961599 67440 layer_factory.hpp:74] Creating layer norm2
I0917 22:16:39.961612 67440 net.cpp:84] Creating Layer norm2
I0917 22:16:39.961622 67440 net.cpp:380] norm2 <- pool2
I0917 22:16:39.961632 67440 net.cpp:338] norm2 -> norm2
I0917 22:16:39.961642 67440 net.cpp:113] Setting up norm2
I0917 22:16:39.961652 67440 net.cpp:120] Top shape: 50 256 13 13 (2163200)
I0917 22:16:39.961658 67440 layer_factory.hpp:74] Creating layer conv3
I0917 22:16:39.961668 67440 net.cpp:84] Creating Layer conv3
I0917 22:16:39.961674 67440 net.cpp:380] conv3 <- norm2
I0917 22:16:39.961685 67440 net.cpp:338] conv3 -> conv3
I0917 22:16:39.961696 67440 net.cpp:113] Setting up conv3
I0917 22:16:39.991262 67440 net.cpp:120] Top shape: 50 384 13 13 (3244800)
I0917 22:16:39.991287 67440 layer_factory.hpp:74] Creating layer relu3
I0917 22:16:39.991299 67440 net.cpp:84] Creating Layer relu3
I0917 22:16:39.991305 67440 net.cpp:380] relu3 <- conv3
I0917 22:16:39.991315 67440 net.cpp:327] relu3 -> conv3 (in-place)
I0917 22:16:39.991324 67440 net.cpp:113] Setting up relu3
I0917 22:16:39.991381 67440 net.cpp:120] Top shape: 50 384 13 13 (3244800)
I0917 22:16:39.991392 67440 layer_factory.hpp:74] Creating layer conv4
I0917 22:16:39.991405 67440 net.cpp:84] Creating Layer conv4
I0917 22:16:39.991411 67440 net.cpp:380] conv4 <- conv3
I0917 22:16:39.991420 67440 net.cpp:338] conv4 -> conv4
I0917 22:16:39.991430 67440 net.cpp:113] Setting up conv4
I0917 22:16:40.013797 67440 net.cpp:120] Top shape: 50 384 13 13 (3244800)
I0917 22:16:40.013823 67440 layer_factory.hpp:74] Creating layer relu4
I0917 22:16:40.013834 67440 net.cpp:84] Creating Layer relu4
I0917 22:16:40.013841 67440 net.cpp:380] relu4 <- conv4
I0917 22:16:40.013850 67440 net.cpp:327] relu4 -> conv4 (in-place)
I0917 22:16:40.013859 67440 net.cpp:113] Setting up relu4
I0917 22:16:40.013914 67440 net.cpp:120] Top shape: 50 384 13 13 (3244800)
I0917 22:16:40.013923 67440 layer_factory.hpp:74] Creating layer conv5
I0917 22:16:40.013936 67440 net.cpp:84] Creating Layer conv5
I0917 22:16:40.013943 67440 net.cpp:380] conv5 <- conv4
I0917 22:16:40.013954 67440 net.cpp:338] conv5 -> conv5
I0917 22:16:40.013965 67440 net.cpp:113] Setting up conv5
I0917 22:16:40.029121 67440 net.cpp:120] Top shape: 50 256 13 13 (2163200)
I0917 22:16:40.029144 67440 layer_factory.hpp:74] Creating layer relu5
I0917 22:16:40.029155 67440 net.cpp:84] Creating Layer relu5
I0917 22:16:40.029162 67440 net.cpp:380] relu5 <- conv5
I0917 22:16:40.029171 67440 net.cpp:327] relu5 -> conv5 (in-place)
I0917 22:16:40.029181 67440 net.cpp:113] Setting up relu5
I0917 22:16:40.029253 67440 net.cpp:120] Top shape: 50 256 13 13 (2163200)
I0917 22:16:40.029263 67440 layer_factory.hpp:74] Creating layer pool5
I0917 22:16:40.029285 67440 net.cpp:84] Creating Layer pool5
I0917 22:16:40.029294 67440 net.cpp:380] pool5 <- conv5
I0917 22:16:40.029302 67440 net.cpp:338] pool5 -> pool5
I0917 22:16:40.029311 67440 net.cpp:113] Setting up pool5
I0917 22:16:40.029454 67440 net.cpp:120] Top shape: 50 256 6 6 (460800)
I0917 22:16:40.029467 67440 layer_factory.hpp:74] Creating layer fc6
I0917 22:16:40.029484 67440 net.cpp:84] Creating Layer fc6
I0917 22:16:40.029492 67440 net.cpp:380] fc6 <- pool5
I0917 22:16:40.029505 67440 net.cpp:338] fc6 -> fc6
I0917 22:16:40.029515 67440 net.cpp:113] Setting up fc6
I0917 22:16:41.249575 67440 net.cpp:120] Top shape: 50 4096 (204800)
I0917 22:16:41.249629 67440 layer_factory.hpp:74] Creating layer relu6
I0917 22:16:41.249647 67440 net.cpp:84] Creating Layer relu6
I0917 22:16:41.249658 67440 net.cpp:380] relu6 <- fc6
I0917 22:16:41.249670 67440 net.cpp:327] relu6 -> fc6 (in-place)
I0917 22:16:41.249681 67440 net.cpp:113] Setting up relu6
I0917 22:16:41.249855 67440 net.cpp:120] Top shape: 50 4096 (204800)
I0917 22:16:41.249866 67440 layer_factory.hpp:74] Creating layer drop6
I0917 22:16:41.249881 67440 net.cpp:84] Creating Layer drop6
I0917 22:16:41.249888 67440 net.cpp:380] drop6 <- fc6
I0917 22:16:41.249899 67440 net.cpp:327] drop6 -> fc6 (in-place)
I0917 22:16:41.249908 67440 net.cpp:113] Setting up drop6
I0917 22:16:41.249925 67440 net.cpp:120] Top shape: 50 4096 (204800)
I0917 22:16:41.249933 67440 layer_factory.hpp:74] Creating layer fc7
I0917 22:16:41.249943 67440 net.cpp:84] Creating Layer fc7
I0917 22:16:41.249950 67440 net.cpp:380] fc7 <- fc6
I0917 22:16:41.249961 67440 net.cpp:338] fc7 -> fc7
I0917 22:16:41.249974 67440 net.cpp:113] Setting up fc7
I0917 22:16:41.792376 67440 net.cpp:120] Top shape: 50 4096 (204800)
I0917 22:16:41.792425 67440 layer_factory.hpp:74] Creating layer relu7
I0917 22:16:41.792443 67440 net.cpp:84] Creating Layer relu7
I0917 22:16:41.792451 67440 net.cpp:380] relu7 <- fc7
I0917 22:16:41.792464 67440 net.cpp:327] relu7 -> fc7 (in-place)
I0917 22:16:41.792476 67440 net.cpp:113] Setting up relu7
I0917 22:16:41.792582 67440 net.cpp:120] Top shape: 50 4096 (204800)
I0917 22:16:41.792593 67440 layer_factory.hpp:74] Creating layer drop7
I0917 22:16:41.792603 67440 net.cpp:84] Creating Layer drop7
I0917 22:16:41.792610 67440 net.cpp:380] drop7 <- fc7
I0917 22:16:41.792619 67440 net.cpp:327] drop7 -> fc7 (in-place)
I0917 22:16:41.792628 67440 net.cpp:113] Setting up drop7
I0917 22:16:41.792637 67440 net.cpp:120] Top shape: 50 4096 (204800)
I0917 22:16:41.792644 67440 layer_factory.hpp:74] Creating layer fc8_oxford_102
I0917 22:16:41.792655 67440 net.cpp:84] Creating Layer fc8_oxford_102
I0917 22:16:41.792662 67440 net.cpp:380] fc8_oxford_102 <- fc7
I0917 22:16:41.792673 67440 net.cpp:338] fc8_oxford_102 -> fc8_oxford_102
I0917 22:16:41.792685 67440 net.cpp:113] Setting up fc8_oxford_102
I0917 22:16:41.806488 67440 net.cpp:120] Top shape: 50 102 (5100)
I0917 22:16:41.806504 67440 layer_factory.hpp:74] Creating layer loss
I0917 22:16:41.806519 67440 net.cpp:84] Creating Layer loss
I0917 22:16:41.806525 67440 net.cpp:380] loss <- fc8_oxford_102
I0917 22:16:41.806532 67440 net.cpp:380] loss <- label
I0917 22:16:41.806551 67440 net.cpp:338] loss -> (automatic)
I0917 22:16:41.806560 67440 net.cpp:113] Setting up loss
I0917 22:16:41.806572 67440 layer_factory.hpp:74] Creating layer loss
I0917 22:16:41.806668 67440 net.cpp:120] Top shape: (1)
I0917 22:16:41.806677 67440 net.cpp:122]     with loss weight 1
I0917 22:16:41.806715 67440 net.cpp:167] loss needs backward computation.
I0917 22:16:41.806722 67440 net.cpp:167] fc8_oxford_102 needs backward computation.
I0917 22:16:41.806728 67440 net.cpp:167] drop7 needs backward computation.
I0917 22:16:41.806735 67440 net.cpp:167] relu7 needs backward computation.
I0917 22:16:41.806740 67440 net.cpp:167] fc7 needs backward computation.
I0917 22:16:41.806746 67440 net.cpp:167] drop6 needs backward computation.
I0917 22:16:41.806764 67440 net.cpp:167] relu6 needs backward computation.
I0917 22:16:41.806771 67440 net.cpp:167] fc6 needs backward computation.
I0917 22:16:41.806787 67440 net.cpp:167] pool5 needs backward computation.
I0917 22:16:41.806793 67440 net.cpp:167] relu5 needs backward computation.
I0917 22:16:41.806802 67440 net.cpp:167] conv5 needs backward computation.
I0917 22:16:41.806809 67440 net.cpp:167] relu4 needs backward computation.
I0917 22:16:41.806818 67440 net.cpp:167] conv4 needs backward computation.
I0917 22:16:41.806823 67440 net.cpp:167] relu3 needs backward computation.
I0917 22:16:41.806829 67440 net.cpp:167] conv3 needs backward computation.
I0917 22:16:41.806836 67440 net.cpp:167] norm2 needs backward computation.
I0917 22:16:41.806843 67440 net.cpp:167] pool2 needs backward computation.
I0917 22:16:41.806850 67440 net.cpp:167] relu2 needs backward computation.
I0917 22:16:41.806855 67440 net.cpp:167] conv2 needs backward computation.
I0917 22:16:41.806861 67440 net.cpp:167] norm1 needs backward computation.
I0917 22:16:41.806870 67440 net.cpp:167] pool1 needs backward computation.
I0917 22:16:41.806879 67440 net.cpp:167] relu1 needs backward computation.
I0917 22:16:41.806885 67440 net.cpp:167] conv1 needs backward computation.
I0917 22:16:41.806891 67440 net.cpp:169] data does not need backward computation.
I0917 22:16:41.806911 67440 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0917 22:16:41.806923 67440 net.cpp:217] Network initialization done.
I0917 22:16:41.806928 67440 net.cpp:218] Memory required for data: 343027604
E0917 22:16:41.807768 67440 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: train_val.prototxt
I0917 22:16:41.807883 67440 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
I0917 22:16:41.807917 67440 solver.cpp:154] Creating test net (#0) specified by net file: train_val.prototxt
I0917 22:16:41.807960 67440 net.cpp:257] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0917 22:16:41.808198 67440 net.cpp:42] Initializing net from parameters: 
name: "Oxford102_CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "/home/ubuntu/caffe/data/ilsvrc12/imagenet_mean.binaryproto"
  }
  image_data_param {
    source: "/home/ubuntu/git/caffe-oxford102/test.txt"
    batch_size: 50
    new_height: 256
    new_width: 256
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_oxford_102"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8_oxford_102"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 102
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_oxford_102"
  bottom: "label"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_oxford_102"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
I0917 22:16:41.808346 67440 layer_factory.hpp:74] Creating layer data
I0917 22:16:41.808361 67440 net.cpp:84] Creating Layer data
I0917 22:16:41.808369 67440 net.cpp:338] data -> data
I0917 22:16:41.808382 67440 net.cpp:338] data -> label
I0917 22:16:41.808393 67440 net.cpp:113] Setting up data
I0917 22:16:41.808400 67440 image_data_layer.cpp:36] Opening file /home/ubuntu/git/caffe-oxford102/test.txt
I0917 22:16:41.811372 67440 image_data_layer.cpp:51] A total of 6149 images.
I0917 22:16:41.815837 67440 image_data_layer.cpp:80] output data size: 50,3,227,227
I0917 22:16:41.815861 67440 data_transformer.cpp:22] Loading mean file from: /home/ubuntu/caffe/data/ilsvrc12/imagenet_mean.binaryproto
I0917 22:16:41.824954 67440 net.cpp:120] Top shape: 50 3 227 227 (7729350)
I0917 22:16:41.824993 67440 net.cpp:120] Top shape: 50 (50)
I0917 22:16:41.825004 67440 layer_factory.hpp:74] Creating layer label_data_1_split
I0917 22:16:41.825037 67440 net.cpp:84] Creating Layer label_data_1_split
I0917 22:16:41.825054 67440 net.cpp:380] label_data_1_split <- label
I0917 22:16:41.825072 67440 net.cpp:338] label_data_1_split -> label_data_1_split_0
I0917 22:16:41.825088 67440 net.cpp:338] label_data_1_split -> label_data_1_split_1
I0917 22:16:41.825096 67440 net.cpp:113] Setting up label_data_1_split
I0917 22:16:41.825112 67440 net.cpp:120] Top shape: 50 (50)
I0917 22:16:41.825120 67440 net.cpp:120] Top shape: 50 (50)
I0917 22:16:41.825126 67440 layer_factory.hpp:74] Creating layer conv1
I0917 22:16:41.825141 67440 net.cpp:84] Creating Layer conv1
I0917 22:16:41.825148 67440 net.cpp:380] conv1 <- data
I0917 22:16:41.825158 67440 net.cpp:338] conv1 -> conv1
I0917 22:16:41.825170 67440 net.cpp:113] Setting up conv1
I0917 22:16:41.826707 67440 net.cpp:120] Top shape: 50 96 55 55 (14520000)
I0917 22:16:41.826730 67440 layer_factory.hpp:74] Creating layer relu1
I0917 22:16:41.826740 67440 net.cpp:84] Creating Layer relu1
I0917 22:16:41.826746 67440 net.cpp:380] relu1 <- conv1
I0917 22:16:41.826756 67440 net.cpp:327] relu1 -> conv1 (in-place)
I0917 22:16:41.826766 67440 net.cpp:113] Setting up relu1
I0917 22:16:41.826905 67440 net.cpp:120] Top shape: 50 96 55 55 (14520000)
I0917 22:16:41.826917 67440 layer_factory.hpp:74] Creating layer pool1
I0917 22:16:41.826930 67440 net.cpp:84] Creating Layer pool1
I0917 22:16:41.826937 67440 net.cpp:380] pool1 <- conv1
I0917 22:16:41.826947 67440 net.cpp:338] pool1 -> pool1
I0917 22:16:41.826956 67440 net.cpp:113] Setting up pool1
I0917 22:16:41.827016 67440 net.cpp:120] Top shape: 50 96 27 27 (3499200)
I0917 22:16:41.827025 67440 layer_factory.hpp:74] Creating layer norm1
I0917 22:16:41.827036 67440 net.cpp:84] Creating Layer norm1
I0917 22:16:41.827044 67440 net.cpp:380] norm1 <- pool1
I0917 22:16:41.827051 67440 net.cpp:338] norm1 -> norm1
I0917 22:16:41.827061 67440 net.cpp:113] Setting up norm1
I0917 22:16:41.827071 67440 net.cpp:120] Top shape: 50 96 27 27 (3499200)
I0917 22:16:41.827077 67440 layer_factory.hpp:74] Creating layer conv2
I0917 22:16:41.827088 67440 net.cpp:84] Creating Layer conv2
I0917 22:16:41.827095 67440 net.cpp:380] conv2 <- norm1
I0917 22:16:41.827105 67440 net.cpp:338] conv2 -> conv2
I0917 22:16:41.827117 67440 net.cpp:113] Setting up conv2
I0917 22:16:41.837579 67440 net.cpp:120] Top shape: 50 256 27 27 (9331200)
I0917 22:16:41.837604 67440 layer_factory.hpp:74] Creating layer relu2
I0917 22:16:41.837613 67440 net.cpp:84] Creating Layer relu2
I0917 22:16:41.837620 67440 net.cpp:380] relu2 <- conv2
I0917 22:16:41.837630 67440 net.cpp:327] relu2 -> conv2 (in-place)
I0917 22:16:41.837638 67440 net.cpp:113] Setting up relu2
I0917 22:16:41.837697 67440 net.cpp:120] Top shape: 50 256 27 27 (9331200)
I0917 22:16:41.837707 67440 layer_factory.hpp:74] Creating layer pool2
I0917 22:16:41.837718 67440 net.cpp:84] Creating Layer pool2
I0917 22:16:41.837723 67440 net.cpp:380] pool2 <- conv2
I0917 22:16:41.837738 67440 net.cpp:338] pool2 -> pool2
I0917 22:16:41.837749 67440 net.cpp:113] Setting up pool2
I0917 22:16:41.837808 67440 net.cpp:120] Top shape: 50 256 13 13 (2163200)
I0917 22:16:41.837818 67440 layer_factory.hpp:74] Creating layer norm2
I0917 22:16:41.837827 67440 net.cpp:84] Creating Layer norm2
I0917 22:16:41.837834 67440 net.cpp:380] norm2 <- pool2
I0917 22:16:41.837846 67440 net.cpp:338] norm2 -> norm2
I0917 22:16:41.837854 67440 net.cpp:113] Setting up norm2
I0917 22:16:41.837864 67440 net.cpp:120] Top shape: 50 256 13 13 (2163200)
I0917 22:16:41.837872 67440 layer_factory.hpp:74] Creating layer conv3
I0917 22:16:41.837882 67440 net.cpp:84] Creating Layer conv3
I0917 22:16:41.837888 67440 net.cpp:380] conv3 <- norm2
I0917 22:16:41.837898 67440 net.cpp:338] conv3 -> conv3
I0917 22:16:41.837906 67440 net.cpp:113] Setting up conv3
I0917 22:16:41.867717 67440 net.cpp:120] Top shape: 50 384 13 13 (3244800)
I0917 22:16:41.867744 67440 layer_factory.hpp:74] Creating layer relu3
I0917 22:16:41.867758 67440 net.cpp:84] Creating Layer relu3
I0917 22:16:41.867766 67440 net.cpp:380] relu3 <- conv3
I0917 22:16:41.867785 67440 net.cpp:327] relu3 -> conv3 (in-place)
I0917 22:16:41.867805 67440 net.cpp:113] Setting up relu3
I0917 22:16:41.867952 67440 net.cpp:120] Top shape: 50 384 13 13 (3244800)
I0917 22:16:41.867965 67440 layer_factory.hpp:74] Creating layer conv4
I0917 22:16:41.867980 67440 net.cpp:84] Creating Layer conv4
I0917 22:16:41.867986 67440 net.cpp:380] conv4 <- conv3
I0917 22:16:41.867996 67440 net.cpp:338] conv4 -> conv4
I0917 22:16:41.868007 67440 net.cpp:113] Setting up conv4
I0917 22:16:41.890466 67440 net.cpp:120] Top shape: 50 384 13 13 (3244800)
I0917 22:16:41.890488 67440 layer_factory.hpp:74] Creating layer relu4
I0917 22:16:41.890498 67440 net.cpp:84] Creating Layer relu4
I0917 22:16:41.890506 67440 net.cpp:380] relu4 <- conv4
I0917 22:16:41.890516 67440 net.cpp:327] relu4 -> conv4 (in-place)
I0917 22:16:41.890525 67440 net.cpp:113] Setting up relu4
I0917 22:16:41.890583 67440 net.cpp:120] Top shape: 50 384 13 13 (3244800)
I0917 22:16:41.890591 67440 layer_factory.hpp:74] Creating layer conv5
I0917 22:16:41.890601 67440 net.cpp:84] Creating Layer conv5
I0917 22:16:41.890609 67440 net.cpp:380] conv5 <- conv4
I0917 22:16:41.890619 67440 net.cpp:338] conv5 -> conv5
I0917 22:16:41.890630 67440 net.cpp:113] Setting up conv5
I0917 22:16:41.905855 67440 net.cpp:120] Top shape: 50 256 13 13 (2163200)
I0917 22:16:41.905879 67440 layer_factory.hpp:74] Creating layer relu5
I0917 22:16:41.905890 67440 net.cpp:84] Creating Layer relu5
I0917 22:16:41.905896 67440 net.cpp:380] relu5 <- conv5
I0917 22:16:41.905905 67440 net.cpp:327] relu5 -> conv5 (in-place)
I0917 22:16:41.905915 67440 net.cpp:113] Setting up relu5
I0917 22:16:41.905972 67440 net.cpp:120] Top shape: 50 256 13 13 (2163200)
I0917 22:16:41.905982 67440 layer_factory.hpp:74] Creating layer pool5
I0917 22:16:41.905997 67440 net.cpp:84] Creating Layer pool5
I0917 22:16:41.906003 67440 net.cpp:380] pool5 <- conv5
I0917 22:16:41.906011 67440 net.cpp:338] pool5 -> pool5
I0917 22:16:41.906021 67440 net.cpp:113] Setting up pool5
I0917 22:16:41.906188 67440 net.cpp:120] Top shape: 50 256 6 6 (460800)
I0917 22:16:41.906203 67440 layer_factory.hpp:74] Creating layer fc6
I0917 22:16:41.906213 67440 net.cpp:84] Creating Layer fc6
I0917 22:16:41.906220 67440 net.cpp:380] fc6 <- pool5
I0917 22:16:41.906230 67440 net.cpp:338] fc6 -> fc6
I0917 22:16:41.906241 67440 net.cpp:113] Setting up fc6
I0917 22:16:43.127418 67440 net.cpp:120] Top shape: 50 4096 (204800)
I0917 22:16:43.127475 67440 layer_factory.hpp:74] Creating layer relu6
I0917 22:16:43.127497 67440 net.cpp:84] Creating Layer relu6
I0917 22:16:43.127509 67440 net.cpp:380] relu6 <- fc6
I0917 22:16:43.127521 67440 net.cpp:327] relu6 -> fc6 (in-place)
I0917 22:16:43.127532 67440 net.cpp:113] Setting up relu6
I0917 22:16:43.127713 67440 net.cpp:120] Top shape: 50 4096 (204800)
I0917 22:16:43.127722 67440 layer_factory.hpp:74] Creating layer drop6
I0917 22:16:43.127735 67440 net.cpp:84] Creating Layer drop6
I0917 22:16:43.127743 67440 net.cpp:380] drop6 <- fc6
I0917 22:16:43.127751 67440 net.cpp:327] drop6 -> fc6 (in-place)
I0917 22:16:43.127760 67440 net.cpp:113] Setting up drop6
I0917 22:16:43.127769 67440 net.cpp:120] Top shape: 50 4096 (204800)
I0917 22:16:43.127776 67440 layer_factory.hpp:74] Creating layer fc7
I0917 22:16:43.127791 67440 net.cpp:84] Creating Layer fc7
I0917 22:16:43.127799 67440 net.cpp:380] fc7 <- fc6
I0917 22:16:43.127810 67440 net.cpp:338] fc7 -> fc7
I0917 22:16:43.127823 67440 net.cpp:113] Setting up fc7
I0917 22:16:43.669579 67440 net.cpp:120] Top shape: 50 4096 (204800)
I0917 22:16:43.669617 67440 layer_factory.hpp:74] Creating layer relu7
I0917 22:16:43.669634 67440 net.cpp:84] Creating Layer relu7
I0917 22:16:43.669643 67440 net.cpp:380] relu7 <- fc7
I0917 22:16:43.669654 67440 net.cpp:327] relu7 -> fc7 (in-place)
I0917 22:16:43.669666 67440 net.cpp:113] Setting up relu7
I0917 22:16:43.669764 67440 net.cpp:120] Top shape: 50 4096 (204800)
I0917 22:16:43.669773 67440 layer_factory.hpp:74] Creating layer drop7
I0917 22:16:43.669791 67440 net.cpp:84] Creating Layer drop7
I0917 22:16:43.669809 67440 net.cpp:380] drop7 <- fc7
I0917 22:16:43.669818 67440 net.cpp:327] drop7 -> fc7 (in-place)
I0917 22:16:43.669836 67440 net.cpp:113] Setting up drop7
I0917 22:16:43.669849 67440 net.cpp:120] Top shape: 50 4096 (204800)
I0917 22:16:43.669857 67440 layer_factory.hpp:74] Creating layer fc8_oxford_102
I0917 22:16:43.669868 67440 net.cpp:84] Creating Layer fc8_oxford_102
I0917 22:16:43.669874 67440 net.cpp:380] fc8_oxford_102 <- fc7
I0917 22:16:43.669883 67440 net.cpp:338] fc8_oxford_102 -> fc8_oxford_102
I0917 22:16:43.669909 67440 net.cpp:113] Setting up fc8_oxford_102
I0917 22:16:43.683697 67440 net.cpp:120] Top shape: 50 102 (5100)
I0917 22:16:43.683714 67440 layer_factory.hpp:74] Creating layer fc8_oxford_102_fc8_oxford_102_0_split
I0917 22:16:43.683727 67440 net.cpp:84] Creating Layer fc8_oxford_102_fc8_oxford_102_0_split
I0917 22:16:43.683733 67440 net.cpp:380] fc8_oxford_102_fc8_oxford_102_0_split <- fc8_oxford_102
I0917 22:16:43.683743 67440 net.cpp:338] fc8_oxford_102_fc8_oxford_102_0_split -> fc8_oxford_102_fc8_oxford_102_0_split_0
I0917 22:16:43.683753 67440 net.cpp:338] fc8_oxford_102_fc8_oxford_102_0_split -> fc8_oxford_102_fc8_oxford_102_0_split_1
I0917 22:16:43.683763 67440 net.cpp:113] Setting up fc8_oxford_102_fc8_oxford_102_0_split
I0917 22:16:43.683771 67440 net.cpp:120] Top shape: 50 102 (5100)
I0917 22:16:43.683779 67440 net.cpp:120] Top shape: 50 102 (5100)
I0917 22:16:43.683785 67440 layer_factory.hpp:74] Creating layer loss
I0917 22:16:43.683794 67440 net.cpp:84] Creating Layer loss
I0917 22:16:43.683800 67440 net.cpp:380] loss <- fc8_oxford_102_fc8_oxford_102_0_split_0
I0917 22:16:43.683807 67440 net.cpp:380] loss <- label_data_1_split_0
I0917 22:16:43.683818 67440 net.cpp:338] loss -> (automatic)
I0917 22:16:43.683827 67440 net.cpp:113] Setting up loss
I0917 22:16:43.683835 67440 layer_factory.hpp:74] Creating layer loss
I0917 22:16:43.683920 67440 net.cpp:120] Top shape: (1)
I0917 22:16:43.683929 67440 net.cpp:122]     with loss weight 1
I0917 22:16:43.683950 67440 layer_factory.hpp:74] Creating layer accuracy
I0917 22:16:43.683965 67440 net.cpp:84] Creating Layer accuracy
I0917 22:16:43.683974 67440 net.cpp:380] accuracy <- fc8_oxford_102_fc8_oxford_102_0_split_1
I0917 22:16:43.683981 67440 net.cpp:380] accuracy <- label_data_1_split_1
I0917 22:16:43.683990 67440 net.cpp:338] accuracy -> accuracy
I0917 22:16:43.683998 67440 net.cpp:113] Setting up accuracy
I0917 22:16:43.684011 67440 net.cpp:120] Top shape: (1)
I0917 22:16:43.684018 67440 net.cpp:169] accuracy does not need backward computation.
I0917 22:16:43.684025 67440 net.cpp:167] loss needs backward computation.
I0917 22:16:43.684031 67440 net.cpp:167] fc8_oxford_102_fc8_oxford_102_0_split needs backward computation.
I0917 22:16:43.684037 67440 net.cpp:167] fc8_oxford_102 needs backward computation.
I0917 22:16:43.684043 67440 net.cpp:167] drop7 needs backward computation.
I0917 22:16:43.684049 67440 net.cpp:167] relu7 needs backward computation.
I0917 22:16:43.684056 67440 net.cpp:167] fc7 needs backward computation.
I0917 22:16:43.684062 67440 net.cpp:167] drop6 needs backward computation.
I0917 22:16:43.684067 67440 net.cpp:167] relu6 needs backward computation.
I0917 22:16:43.684073 67440 net.cpp:167] fc6 needs backward computation.
I0917 22:16:43.684079 67440 net.cpp:167] pool5 needs backward computation.
I0917 22:16:43.684087 67440 net.cpp:167] relu5 needs backward computation.
I0917 22:16:43.684092 67440 net.cpp:167] conv5 needs backward computation.
I0917 22:16:43.684099 67440 net.cpp:167] relu4 needs backward computation.
I0917 22:16:43.684105 67440 net.cpp:167] conv4 needs backward computation.
I0917 22:16:43.684111 67440 net.cpp:167] relu3 needs backward computation.
I0917 22:16:43.684118 67440 net.cpp:167] conv3 needs backward computation.
I0917 22:16:43.684124 67440 net.cpp:167] norm2 needs backward computation.
I0917 22:16:43.684131 67440 net.cpp:167] pool2 needs backward computation.
I0917 22:16:43.684139 67440 net.cpp:167] relu2 needs backward computation.
I0917 22:16:43.684150 67440 net.cpp:167] conv2 needs backward computation.
I0917 22:16:43.684157 67440 net.cpp:167] norm1 needs backward computation.
I0917 22:16:43.684175 67440 net.cpp:167] pool1 needs backward computation.
I0917 22:16:43.684182 67440 net.cpp:167] relu1 needs backward computation.
I0917 22:16:43.684188 67440 net.cpp:167] conv1 needs backward computation.
I0917 22:16:43.684195 67440 net.cpp:169] label_data_1_split does not need backward computation.
I0917 22:16:43.684201 67440 net.cpp:169] data does not need backward computation.
I0917 22:16:43.684206 67440 net.cpp:205] This network produces output accuracy
I0917 22:16:43.684227 67440 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0917 22:16:43.684237 67440 net.cpp:217] Network initialization done.
I0917 22:16:43.684242 67440 net.cpp:218] Memory required for data: 343068808
I0917 22:16:43.684412 67440 solver.cpp:42] Solver scaffolding done.
I0917 22:16:43.684468 67440 caffe.cpp:86] Finetuning from pretrained-weights.caffemodel
E0917 22:16:44.109917 67440 upgrade_proto.cpp:609] Attempting to upgrade input file specified using deprecated transformation parameters: pretrained-weights.caffemodel
I0917 22:16:44.109978 67440 upgrade_proto.cpp:612] Successfully upgraded file specified using deprecated data transformation parameters.
E0917 22:16:44.109985 67440 upgrade_proto.cpp:614] Note that future Caffe releases will only support transform_param messages for transformation fields.
E0917 22:16:44.109997 67440 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: pretrained-weights.caffemodel
I0917 22:16:44.300703 67440 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
E0917 22:16:44.779665 67440 upgrade_proto.cpp:609] Attempting to upgrade input file specified using deprecated transformation parameters: pretrained-weights.caffemodel
I0917 22:16:44.779745 67440 upgrade_proto.cpp:612] Successfully upgraded file specified using deprecated data transformation parameters.
E0917 22:16:44.779779 67440 upgrade_proto.cpp:614] Note that future Caffe releases will only support transform_param messages for transformation fields.
E0917 22:16:44.779793 67440 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: pretrained-weights.caffemodel
I0917 22:16:44.985101 67440 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
I0917 22:16:45.036567 67440 solver.cpp:222] Solving Oxford102_CaffeNet
I0917 22:16:45.036615 67440 solver.cpp:223] Learning Rate Policy: step
I0917 22:16:45.036629 67440 solver.cpp:266] Iteration 0, Testing net (#0)
I0917 22:17:11.646560 67440 solver.cpp:315]     Test net output #0: accuracy = 0.0135484
I0917 22:17:11.817837 67440 solver.cpp:189] Iteration 0, loss = 5.32372
I0917 22:17:11.817905 67440 solver.cpp:464] Iteration 0, lr = 0.001
I0917 22:17:31.788559 67440 solver.cpp:189] Iteration 50, loss = 0.903466
I0917 22:17:31.788626 67440 solver.cpp:464] Iteration 50, lr = 0.001
I0917 22:17:51.749511 67440 solver.cpp:189] Iteration 100, loss = 0.144536
I0917 22:17:51.749843 67440 solver.cpp:464] Iteration 100, lr = 0.001
I0917 22:18:11.730101 67440 solver.cpp:189] Iteration 150, loss = 0.181772
I0917 22:18:11.730164 67440 solver.cpp:464] Iteration 150, lr = 0.001
I0917 22:18:31.702170 67440 solver.cpp:189] Iteration 200, loss = 0.104574
I0917 22:18:31.702463 67440 solver.cpp:464] Iteration 200, lr = 0.001
I0917 22:18:51.669788 67440 solver.cpp:189] Iteration 250, loss = 0.0348623
I0917 22:18:51.669863 67440 solver.cpp:464] Iteration 250, lr = 0.001
I0917 22:19:11.627059 67440 solver.cpp:189] Iteration 300, loss = 0.138429
I0917 22:19:11.627261 67440 solver.cpp:464] Iteration 300, lr = 0.001
I0917 22:19:31.595577 67440 solver.cpp:189] Iteration 350, loss = 0.0196265
I0917 22:19:31.595651 67440 solver.cpp:464] Iteration 350, lr = 0.001
I0917 22:19:51.542239 67440 solver.cpp:189] Iteration 400, loss = 0.0481769
I0917 22:19:51.542544 67440 solver.cpp:464] Iteration 400, lr = 0.001
I0917 22:20:11.507810 67440 solver.cpp:189] Iteration 450, loss = 0.00802259
I0917 22:20:11.507877 67440 solver.cpp:464] Iteration 450, lr = 0.001
I0917 22:20:31.077005 67440 solver.cpp:266] Iteration 500, Testing net (#0)
I0917 22:20:57.083693 67440 solver.cpp:315]     Test net output #0: accuracy = 0.732419
I0917 22:20:57.233280 67440 solver.cpp:189] Iteration 500, loss = 0.0672842
I0917 22:20:57.233338 67440 solver.cpp:464] Iteration 500, lr = 0.001
